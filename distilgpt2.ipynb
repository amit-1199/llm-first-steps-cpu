{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvvgxqQ0GazPxwBQ2rUMta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amit-1199/llm-first-steps-cpu/blob/main/distilgpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OXOPAiJRyJd",
        "outputId": "b8ade3e6-b3cf-4e25-f1ff-b6fe55689ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference for prompt: Give me three practical study tips for learning Python fast.\n",
            "Output:\n",
            "Give me three practical study tips for learning Python fast.\n",
            "\n",
            "1. Set the example to a few lines\n",
            "2. You can use this to test the code:\n",
            "import \"python.py\" import \"py.python\" # If you're a Python programmer, you can learn this in the Python 3.0.1 (see the manual for more details)\n",
            "3. Use the examples to show how to learn Python faster. If it's not, use the following examples. # # Python 2.2 will be the best Python tutorial I've ever written.\n"
          ]
        }
      ],
      "source": [
        "# This script shows how to download a text-generation model from Hugging Face\n",
        "# and run prompts using only CPU (no CUDA/GPU required).\n",
        "#\n",
        "# Why this approach?\n",
        "# Many institutes and tutorials provide code that requires CUDA (GPU) and expect you to use Google Colab.\n",
        "# However, not everyone explains how to run these models on your own computer using only CPU.\n",
        "# This script is for those who want to experiment locally, without needing a GPU or Colab.\n",
        "#\n",
        "# Steps:\n",
        "# 1. Choose a model from Hugging Face (here we use \"distilgpt2\" for speed and low resource usage).\n",
        "# 2. Download the tokenizer and model using AutoTokenizer and AutoModelForCausalLM.\n",
        "# 3. Create a text-generation pipeline.\n",
        "# 4. Run your prompt and print the generated output.\n",
        "#\n",
        "# You can change MODEL_ID to any supported Hugging Face model.\n",
        "# For larger models, ensure you have enough RAM and compute resources.\n",
        "# For interactive use, uncomment the loop at the end.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# Configure basic logging to display information during execution\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize model and tokenizer (these can be initialized once)\n",
        "# Choose a model from Hugging Face (here we use \"distilgpt2\" for speed and low resource usage)\n",
        "MODEL_ID = \"distilgpt2\"\n",
        "logger.info(f\"Loading tokenizer for model: {MODEL_ID}\")\n",
        "# Download and load the tokenizer associated with the chosen model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "logger.info(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "logger.info(f\"Loading model: {MODEL_ID}\")\n",
        "# Download and load the pre-trained language model\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "logger.info(\"Model loaded successfully.\")\n",
        "\n",
        "# Create text-generation pipeline\n",
        "# This simplifies the process of text generation (tokenization, model inference, decoding)\n",
        "logger.info(\"Creating text-generation pipeline.\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,  # Connect the loaded model\n",
        "    tokenizer=tokenizer,  # Connect the loaded tokenizer\n",
        "    device=-1  # Use CPU (-1 indicates CPU, 0 or higher would typically indicate a GPU)\n",
        ")\n",
        "logger.info(\"Pipeline created successfully.\")\n",
        "\n",
        "# Add a check to ensure the pipeline is callable\n",
        "if not callable(pipe):\n",
        "    logger.error(\"Text generation pipeline is not callable. There might be an issue with initialization.\")\n",
        "    # You might want to add more error handling or exit here if the pipeline is not usable\n",
        "\n",
        "def run_prompt(prompt):\n",
        "    # Print the prompt being processed\n",
        "    print(f\"Running inference for prompt: {prompt}\")\n",
        "    if callable(pipe):\n",
        "        # Run the text generation using the configured pipeline\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=250,  # Limit the maximum number of new tokens to generate\n",
        "            do_sample=True,  # Enable sampling-based text generation for varied outputs\n",
        "            temperature=0.7,  # Control the randomness of sampling (lower = less random, higher = more random)\n",
        "            top_p=0.9,  # Use nucleus sampling (consider tokens whose cumulative probability exceeds 0.9)\n",
        "            pad_token_id=tokenizer.eos_token_id, # Specify the token ID to use for padding\n",
        "            no_repeat_ngram_size=2, # Prevent repeating n-grams of this size\n",
        "        )\n",
        "        print(\"Output:\")\n",
        "        # Extract and print the generated text from the output\n",
        "        print(out[0][\"generated_text\"])\n",
        "    else:\n",
        "        print(\"Pipeline is not callable. Cannot run inference.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    prompt = \"Give me three practical study tips for learning Python fast.\"\n",
        "    run_prompt(prompt)\n",
        "\n",
        "\n",
        "    # Uncomment below for interactive prompt loop\n",
        "    # while True:\n",
        "    #     prompt = input(\"Enter your prompt (or 'exit' to quit): \")\n",
        "    #     if prompt.lower() == \"exit\":\n",
        "    #         break\n",
        "    #     run_prompt(prompt)"
      ]
    }
  ]
}